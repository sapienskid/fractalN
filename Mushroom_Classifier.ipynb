{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mushroom Classification with Deep Learning\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/FractalN/blob/main/Mushroom_Classifier.ipynb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset found in local environment.\n"
          ]
        }
      ],
      "source": [
        "# Check if we're running in Colab\n",
        "import os\n",
        "from pathlib import Path\n",
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Clone repository (includes dataset in data/ folder)\n",
        "    !git clone https://github.com/YOUR_USERNAME/FractalN.git\n",
        "    %cd FractalN\n",
        "    \n",
        "    # Install additional requirements\n",
        "    !pip install -r requirements.txt\n",
        "    \n",
        "    # Verify dataset is present\n",
        "    !ls -R data/\n",
        "    \n",
        "    # Mount Google Drive for saving results\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Create results directory in Drive\n",
        "    !mkdir -p \"/content/drive/MyDrive/FractalN_Results\"\n",
        "else:\n",
        "    # Verify dataset is present in local environment\n",
        "    if not os.path.exists('data'):\n",
        "        raise FileNotFoundError(\"Dataset not found! Please ensure 'data' directory exists.\")\n",
        "    print(\"Dataset found in local environment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected CUDA version: 12.6\n",
            "\n",
            "NVIDIA GPU Information:\n",
            "Thu Dec 26 07:30:23 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650        On  |   00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   44C    P0             14W /   50W |     369MiB /   4096MiB |      3%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      1763      G   /usr/bin/gnome-shell                            1MiB |\n",
            "|    0   N/A  N/A    879436    C+G   /usr/bin/kgx                                   44MiB |\n",
            "|    0   N/A  N/A   1239353      G   ...erProcess --variations-seed-version        106MiB |\n",
            "|    0   N/A  N/A   1242702      G   ...seed-version=20241219-130728.147000         83MiB |\n",
            "|    0   N/A  N/A   1311071      C   ...velopment/fractalN/.venv/bin/python         64MiB |\n",
            "|    0   N/A  N/A   1312219      C   ...velopment/fractalN/.venv/bin/python         64MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "Using CUDA RT library: /opt/cuda/targets/x86_64-linux/lib/libcudart.so.12.6.77\n",
            "\n",
            "Found 1 GPU(s)\n",
            "Enabled memory growth for /physical_device:GPU:0\n",
            "\n",
            "GPU test result: [[ 7. 10.]\n",
            " [15. 22.]]\n",
            "Using device: /job:localhost/replica:0/task:0/device:GPU:0\n",
            "TensorFlow version: 2.18.0\n",
            "GPU devices: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "# Setup environment\n",
        "import os\n",
        "import sys\n",
        "from src.gpu_config import setup_gpu\n",
        "import tensorflow as tf\n",
        "\n",
        "# Setup GPU\n",
        "setup_gpu()\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU devices:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Set results directory based on environment\n",
        "RESULTS_DIR = '/content/drive/MyDrive/FractalN_Results' if IN_COLAB else 'results'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths and check for cached processed data\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # Define paths\n",
        "    DRIVE_ROOT = '/content/drive/MyDrive/FractalN_Data'\n",
        "    AUGMENTED_DATA_PATH = f\"{DRIVE_ROOT}/final_processed_data.zip\"  # Final processed data after all steps\n",
        "    RESULTS_DIR = f\"{DRIVE_ROOT}/Results\"\n",
        "    \n",
        "    # Create directories\n",
        "    !mkdir -p \"{DRIVE_ROOT}\"\n",
        "    !mkdir -p \"{RESULTS_DIR}\"\n",
        "    \n",
        "    # Check if final processed data exists in Drive\n",
        "    NEED_PROCESSING = not os.path.exists(AUGMENTED_DATA_PATH)\n",
        "else:\n",
        "    RESULTS_DIR = 'results'\n",
        "    NEED_PROCESSING = not os.path.exists('data/processed')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Complete Data Pipeline\n",
        "Process data in three steps:\n",
        "1. Organize raw data\n",
        "2. Augment organized data\n",
        "3. Preprocess augmented data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found cached processed data. Loading...\n",
            "Cached data loaded successfully!\n",
            "/usr/bin/bash: line 1: tree: command not found\n",
            "\n",
            "Final dataset distribution:\n",
            "\n",
            "Train set:\n",
            "poisonous: 1627 images\n",
            "edible: 1864 images\n",
            "\n",
            "Test set:\n",
            "poisonous: 338 images\n",
            "edible: 387 images\n",
            "\n",
            "Val set:\n",
            "poisonous: 288 images\n",
            "edible: 329 images\n"
          ]
        }
      ],
      "source": [
        "if NEED_PROCESSING:\n",
        "    print(\"No cached processed data found. Starting complete data pipeline...\")\n",
        "    \n",
        "    # Step 1: Organize raw data\n",
        "    print(\"\\n1. Organizing raw data...\")\n",
        "    from utils.reorganize_data import reorganize_mushroom_data\n",
        "    reorganize_mushroom_data()\n",
        "    \n",
        "    # Step 2: Augment organized data\n",
        "    print(\"\\n2. Augmenting organized data...\")\n",
        "    from utils.augment_mushroom_data import augment_mushroom_data\n",
        "    augment_mushroom_data(target_count=20000)\n",
        "    \n",
        "    # Step 3: Preprocess augmented data\n",
        "    print(\"\\n3. Preprocessing augmented data...\")\n",
        "    from utils.preprocess_data import preprocess_dataset\n",
        "    preprocess_dataset(\n",
        "        data_dir='data/mushroom_data',\n",
        "        output_dir='data/processed',\n",
        "        test_size=0.2,\n",
        "        img_size=(224, 224)\n",
        "    )\n",
        "    \n",
        "    # Cache the final processed data\n",
        "    if IN_COLAB:\n",
        "        print(\"\\nSaving final processed data to Google Drive...\")\n",
        "        !zip -r \"{AUGMENTED_DATA_PATH}\" data/processed\n",
        "        print(f\"Final processed data saved to: {AUGMENTED_DATA_PATH}\")\n",
        "else:\n",
        "    print(\"Found cached processed data. Loading...\")\n",
        "    if IN_COLAB:\n",
        "        !unzip -q \"{AUGMENTED_DATA_PATH}\" -d \"data/\"\n",
        "    print(\"Cached data loaded successfully!\")\n",
        "\n",
        "# Verify final data structure and distribution\n",
        "!tree data/processed -L 3\n",
        "\n",
        "print(\"\\nFinal dataset distribution:\")\n",
        "for split in ['train', 'test', 'val']:\n",
        "    print(f\"\\n{split.capitalize()} set:\")\n",
        "    for category in ['poisonous', 'edible']:\n",
        "        count = len(list(Path(f'data/processed/{split}/{category}').glob('*.jpg')))\n",
        "        print(f\"{category}: {count} images\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Training\n",
        "Train using the fully processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added /mnt/external/Development/fractalN to Python path\n",
            "Detected CUDA version: 12.6\n",
            "\n",
            "NVIDIA GPU Information:\n",
            "Thu Dec 26 07:33:31 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 565.77                 Driver Version: 565.77         CUDA Version: 12.7     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce GTX 1650        On  |   00000000:01:00.0 Off |                  N/A |\n",
            "| N/A   44C    P0             13W /   50W |     352MiB /   4096MiB |     30%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      1763      G   /usr/bin/gnome-shell                            1MiB |\n",
            "|    0   N/A  N/A    879436    C+G   /usr/bin/kgx                                   44MiB |\n",
            "|    0   N/A  N/A   1239353      G   ...erProcess --variations-seed-version        108MiB |\n",
            "|    0   N/A  N/A   1242702      G   ...seed-version=20241219-130728.147000         63MiB |\n",
            "|    0   N/A  N/A   1311071      C   ...velopment/fractalN/.venv/bin/python         64MiB |\n",
            "|    0   N/A  N/A   1312219      C   ...velopment/fractalN/.venv/bin/python         64MiB |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "Using CUDA RT library: /opt/cuda/targets/x86_64-linux/lib/libcudart.so.12.6.77\n",
            "\n",
            "Found 1 GPU(s)\n",
            "Enabled memory growth for /physical_device:GPU:0\n",
            "\n",
            "GPU test result: [[ 7. 10.]\n",
            " [15. 22.]]\n",
            "Using device: /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Found existing dataset: 1864 edible, 1627 poisonous\n",
            "Found 1864 edible and 1627 poisonous training images\n",
            "Found 3491 files belonging to 2 classes.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/mnt/external/Development/fractalN/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 617 files belonging to 2 classes.\n",
            "Found 725 files belonging to 2 classes.\n",
            "\n",
            "Class weights being used:\n",
            "Class 0: 0.9364\n",
            "Class 1: 1.0728\n",
            "Epoch 1/150\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1735177747.824846 1311071 cuda_dnn.cc:529] Loaded cuDNN version 90501\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error during training: Exception encountered when calling Dropout.call().\n",
            "\n",
            "\u001b[1m{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: \u001b[0m\n",
            "\n",
            "Arguments received by Dropout.call():\n",
            "  • inputs=tf.Tensor(shape=(8, 37, 37, 256), dtype=float16)\n",
            "  • training=True\n",
            "\n",
            "Please ensure you have:\n",
            "1. Uploaded your data correctly\n",
            "2. Run with preprocess=True for first time setup\n",
            "3. Have the correct directory structure\n"
          ]
        },
        {
          "ename": "ResourceExhaustedError",
          "evalue": "Exception encountered when calling Dropout.call().\n\n\u001b[1m{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: \u001b[0m\n\nArguments received by Dropout.call():\n  • inputs=tf.Tensor(shape=(8, 37, 37, 256), dtype=float16)\n  • training=True",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[30], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_model\n\u001b[1;32m     35\u001b[0m need_preprocess \u001b[38;5;241m=\u001b[39m verify_and_setup()\n\u001b[0;32m---> 36\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_preprocess\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDisplaying training results...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
            "File \u001b[0;32m/mnt/external/Development/fractalN/src/train.py:360\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(preprocess)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# Evaluate and save results\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEvaluating on test set:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/mnt/external/Development/fractalN/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m/mnt/external/Development/fractalN/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling Dropout.call().\n\n\u001b[1m{{function_node __wrapped__Mul_device_/job:localhost/replica:0/task:0/device:GPU:0}} failed to allocate memory [Op:Mul] name: \u001b[0m\n\nArguments received by Dropout.call():\n  • inputs=tf.Tensor(shape=(8, 37, 37, 256), dtype=float16)\n  • training=True"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "def verify_and_setup():\n",
        "    \"\"\"Verify data structure and setup for training\"\"\"\n",
        "    if not os.path.exists('data/processed'):\n",
        "        print(\"No processed data found. Will run preprocessing.\")\n",
        "        return True\n",
        "    \n",
        "    train_dir = Path('data/processed/train')\n",
        "    if not train_dir.exists():\n",
        "        print(\"No training directory found. Will run preprocessing.\")\n",
        "        return True\n",
        "        \n",
        "    edible_count = len(list((train_dir / 'edible').glob('*.[Jj][Pp][Gg]')))\n",
        "    poisonous_count = len(list((train_dir / 'poisonous').glob('*.[Jj][Pp][Gg]')))\n",
        "    \n",
        "    if edible_count == 0 or poisonous_count == 0:\n",
        "        print(\"Missing data in training directories. Will run preprocessing.\")\n",
        "        return True\n",
        "        \n",
        "    print(f\"Found existing dataset: {edible_count} edible, {poisonous_count} poisonous\")\n",
        "    return False\n",
        "\n",
        "# Setup Python path for imports\n",
        "import os\n",
        "import sys\n",
        "notebook_dir = os.path.abspath('')\n",
        "src_dir = os.path.join(notebook_dir, 'src')\n",
        "if src_dir not in sys.path:\n",
        "    sys.path.append(notebook_dir)\n",
        "    print(f\"Added {notebook_dir} to Python path\")\n",
        "\n",
        "from src.train import train_model\n",
        "from src.model import create_model\n",
        "need_preprocess = verify_and_setup()\n",
        "model, history = train_model(preprocess=need_preprocess)\n",
        "\n",
        "print(\"\\nDisplaying training results...\")\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "img = mpimg.imread('training_history.png')\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTraining Metrics:\")\n",
        "with open('training_metrics.txt', 'r') as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation and Prediction\n",
        "Test the trained model on sample images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.predict import predict_mushroom\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "def test_random_images(num_samples=3):\n",
        "    test_dir = Path('data/processed/test')\n",
        "    \n",
        "    for category in ['edible', 'poisonous']:\n",
        "        print(f\"\\nTesting {category} mushrooms:\")\n",
        "        category_path = test_dir / category\n",
        "        image_files = list(category_path.glob('*.[Jj][Pp][Gg]'))\n",
        "        \n",
        "        for _ in range(num_samples):\n",
        "            test_image = random.choice(image_files)\n",
        "            print(f\"\\nImage: {test_image.name}\")\n",
        "            prediction, confidence = predict_mushroom(\n",
        "                'mushroom_classifier.keras',\n",
        "                str(test_image)\n",
        "            )\n",
        "            print(f\"Predicted: {prediction}\")\n",
        "            print(f\"Confidence: {confidence:.2%}\")\n",
        "\n",
        "# Test model on random images\n",
        "test_random_images()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Model and Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to Google Drive if in Colab\n",
        "if IN_COLAB:\n",
        "    !cp mushroom_classifier.keras \"{RESULTS_DIR}/\"\n",
        "    !cp best_model.keras \"{RESULTS_DIR}/\"\n",
        "    !cp training_history.png \"{RESULTS_DIR}/\"\n",
        "    !cp training_metrics.txt \"{RESULTS_DIR}/\"\n",
        "    !cp training_log.csv \"{RESULTS_DIR}/\"\n",
        "    print(f\"Model and results saved to Google Drive: {RESULTS_DIR}\")\n",
        "else:\n",
        "    !mkdir -p results\n",
        "    !cp mushroom_classifier.keras results/\n",
        "    !cp best_model.keras results/\n",
        "    !cp training_history.png results/\n",
        "    !cp training_metrics.txt results/\n",
        "    !cp training_log.csv results/\n",
        "    print(\"Model and results saved in 'results' directory\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Mushroom_Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
